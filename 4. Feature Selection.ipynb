{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 1639)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from usfull_tools import load_DS\n",
    "from catboost import Pool, CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_columns = None\n",
    "%matplotlib inline\n",
    "\n",
    "from set_vars import KAGGLE_PREFIX, debug_mode, KAGGLE_DIR, target_column, target_type, loss_function, custom_metric\n",
    "\n",
    "train, test = load_DS(debug_mode, KAGGLE_DIR, KAGGLE_PREFIX, '_prepare.csv')\n",
    "del test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[train.columns.drop(target_column)], train[target_column], \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations : 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Custom metrics will not be evaluated because there are no test datasets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 179874.9263887\ttotal: 421ms\tremaining: 41.7s\n",
      "1:\tlearn: 164527.6039938\ttotal: 4.99s\tremaining: 4m 4s\n",
      "2:\tlearn: 150312.8566030\ttotal: 10.6s\tremaining: 5m 42s\n",
      "3:\tlearn: 138087.9104594\ttotal: 13.7s\tremaining: 5m 29s\n",
      "4:\tlearn: 126783.2697156\ttotal: 17s\tremaining: 5m 23s\n",
      "5:\tlearn: 116253.2012594\ttotal: 19.7s\tremaining: 5m 8s\n"
     ]
    }
   ],
   "source": [
    "iterations = np.round(10+len(X_train.columns)/2)\n",
    "iterations = 100\n",
    "\n",
    "\n",
    "print('iterations :', iterations)\n",
    "\n",
    "if target_type=='binary':\n",
    "    model = CatBoostClassifier(random_seed = 42, iterations=iterations, depth=2, learning_rate=0.1, \n",
    "                               loss_function=loss_function, custom_metric=custom_metric, od_type = 'Iter'\n",
    "#                               , task_type='GPU', devices='0'\n",
    "                              )\n",
    "elif target_type=='interval':\n",
    "    model = CatBoostRegressor(random_seed = 42, iterations=iterations, depth=6, learning_rate=0.1, \n",
    "                              loss_function=loss_function, custom_metric=custom_metric, od_type = 'Iter'\n",
    "#                                , task_type='GPU', devices='0'\n",
    "                              )\n",
    "    \n",
    "#https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_parameters-list-docpage/#python-reference_parameters-list\n",
    "\n",
    "# Для CatBoost требуется явно указывать категориальные переменные\n",
    "i=0\n",
    "cat_features = []\n",
    "for column in X_train.columns:\n",
    "    if X_train[column].dtype == 'object': cat_features.append(i)\n",
    "    i +=1\n",
    "\n",
    "model.fit(X_train, y_train, cat_features)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "\n",
    "if custom_metric=='Accuracy':\n",
    "    print(\"Accuracy: %.3f\"\n",
    "          % accuracy_score(model.predict(X_test), y_test))\n",
    "\n",
    "if custom_metric=='RMSE':\n",
    "    print(\"RMSE: %.3f\"\n",
    "          % mean_absolute_error(model.predict(X_test), y_test))\n",
    "\n",
    "\n",
    "feature_importance = pd.DataFrame(list(zip(X_test.dtypes.index, \n",
    "                                           model.get_feature_importance(Pool(X_test, label=y_test, cat_features=cat_features)))),\n",
    "                                    columns=['Feature','Score'])\n",
    "\n",
    "feature_importance = feature_importance.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "#TO DO: сделать отбор лучшего из нескольких корелирующих параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep features with > 1% normalize importance\n",
    "fi = feature_importance[feature_importance.Score > 1]\n",
    "print(len(X_train.columns), '->', len(fi.index), 'non zero important features:', np.round(fi.Score.sum(),1), '%')\n",
    "fi.sort_values('Score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi.to_csv(KAGGLE_DIR + KAGGLE_PREFIX + '_important_columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
